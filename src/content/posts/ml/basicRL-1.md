---
title: RL 学习 —— Basic
published: 2025-07-10
description: 个人认为 RL 是可解释性相当优秀的自进化方法, 值得学习, 未来可能也想结合 MLLMs/Agent + RL 方向. 这篇是基础知识.
tags: [ML, RL, DL]
category: ML
draft: false
---

记录新坑🥹, RL(Reinforcement Learning)/Agent 学习, 
个人认为 RL 是可解释性相当优秀的自进化方法, 值得学习, 未来可能也想结合 MLLMs/Agent + RL 方向.

<!--more-->

## 个人理解

从我个人的理解来说, RL 就是自进化算法, 通过预先设定的一些 "规则" 让智能体 (agent) 自己学会最优解. 例如下五子棋、走迷宫等等, 智能体会在每个时间点 (时间步) 根据当前环境状态 (state) 按照一定的策略 (policy) 与环境发生交互, 并最终尽可能取得全局最优解.

## 学术一点

### 马尔可夫决策

一个更学术的观点, RL 是一个马尔可夫决策过程, 由四元组 $(S, A, P_a, R_a)$ 组成

- $S$: 状态空间, 假设某个时间步对应的状态是 $a_t$, 则 $S = (s_0, ..., s_k)$.
- $A$: 动作空间, 假设某个时间步对应的状态是 $a_t$, 则 $A = (a_0, ..., a_k)$.
- $P_a$: 即 $"Pr"(s_(t+1)|s_t, a_t)$, 代表某个时间步在动作 $a_t$ 影响下从 $s_t$ 到 $s_(t+1)$ 的概率.
- $R_a$: 即 $"Reward"(s_t, a_t)$, 代表某个状态执行某个动作的奖励.

### 策略

我们的目标是让智能体学会最优解, 即最大化 $sum R_a$. 此时我们的智能体会按照某个策略 $pi$ 根据不同状态决定不同的动作. 此时 $pi$ 代表了一个映射, 其输入为状态空间与动作空间笛卡尔积, 输出则是概率, 也就是在 $t$ 时刻状态为 $s_t$ 的条件下采用动作 $a_t$ 的概率.

$$
  pi : S times A -> [0,1]
$$

$$
  pi(s, a) = "Pr"(a_t|s_t)
$$

### 奖励

为了衡量总的奖励, 我们需要在某个时刻点 $t$ 计算未来能获得多少奖励, 此时可以将未来获得的回报都加起来, 设为 $G_t$ .

$$
  G_t = sum^(infinity)_(t) R_(t+1)
$$

但是这造成了一个问题, 可以立刻获得的奖励 (即时奖励) 是比未来的奖励更重要的, 所以我们引入一个折扣因子 $gamma in [0, 1]$ 将 $G_t$ 改写.

$$
  G_t = sum^(infinity)_(k=0) gamma^(k) R_(t+k+1)
$$

### 价值

为了更好地评估某个策略 $pi$ 在不同状态或采取动作下未来能得到多少奖励, 我们需要了解在策略 $pi$ 的条件下, 处于某状态 $s_t$ 的价值 (状态价值)、处于某状态 $s_t$ 之后采取动作 $a_t$ 的价值 (动作价值). 策略 $pi$ 是一个概率分布, 因此我们应该采用期望来建模价值.

- 状态价值 $V_(pi)(s_t)=EE_(pi)(G_t | s_t)$, 对应在某策略下某状态的期望.
- 动作价值 $Q_(pi)(s_t, a_t)=EE_(pi)(G_t | s_t, a_t)$, 对应在某策略下某状态采取某动作的期望.

由于 $pi(s_t, a_t) = "Pr"(a_t|s_t)$, 因此可以得到状态价值和动作价值的关系.

$$
  V_(pi)(s_t) = sum^(A_t)_(a_t) pi(s_t, a_t) times Q_(pi)(s_t, a_t)
$$

### 最优化目标

RL 的基本目标就是尽可能得到最高的全局 Reward. 假设全局最优用 $*$ 上标表示.

- 对于状态价值, 我们选择所有策略中最大的.

$$
  V^(*)_(pi)(s_t) = max_(pi) V_(pi)(s_t)
$$

- 对于动作价值, 我们同样可以每次都选择最大的, 但是我们会考虑从所有动作中选择能提供最大价值的函数, 那么由于该情况下 $pi$ 是确定的, 所以依然能通过 $Q^(*)$ 与 $pi$ 得到 $V^(*)$.

$$
  Q^(*)_(pi)(s_t, dot.c) = max_(pi, a_t in A) Q_(pi)(s_t, a_t)
$$


## 五子棋

一个更具体的例子, 让我们以 10x10 棋盘下一把五子棋. 智能体需要学习到的就是根据当前所有棋在棋盘上的情况 (state), 学会下一步 (时间步) 应该往哪下棋 (policy) 让智能体取得胜利. 为了达到这个过程, 我们设计一些奖励 (reward), 当智能体下棋时连成了二子、三子等等就给予一些奖励, 而当智能体拦截了对手让对手不能连成二子、三子等等也给予一些奖励, 然后可以让两个智能体之间相互下棋, 或者和人类下棋, 让智能体学会这个过程.

### 总时间步

对于一场棋局, 显然当落满棋子一定结束, 所以最多有 100 个时间步, 但是对于两个对弈的智能体来说, 每个智能体相互落子, 所以时间步应该减半. 此时, 我们认为 50 个时间步就是一个完整的 eposide.

### 状态空间

在该场景下, 我们可能将棋局的所有坐标建模为状态, 例如 $(x, y, "exist", "color")$ 代表第 $x$ 行 $y$ 列的棋子是 $"color"$ 色的如果 $"exist"$ 的话. 假设 $"color"$ 和 $"exist"$ 为二值的, 则状态空间应该有 $10 times 10 times 2 times 2$ 这么大.

### 动作空间

假设我们有两个互相博弈的智能体, 每个智能体只下黑棋或者白棋, 那么对于某个智能体来说, 我们的动作其实就是 $(x, y)$, 表示在第 $x$ 行 $y$ 列落子. 总动作空间为 $10 times 10$, 当然并非固定, 动作空间大小应该随落子数量变多而减少.

### 奖励

奖励 (Reward Model) 的设计较为灵活, 我们可能会这样设计.

- 当连成 2 子时 +1; 3 子 +2; 4 子 +3; 5 子 +6;
- 当阻止对方连成 2、3、4、5 子 时 +1, +2, +3, +5;
- 当尝试落子的位置已经存在棋子时 -1;
- ...

实际设计时可以尝试让 "连子" 的奖励略高于 "阻止", 也可以反过来, 但是最好让连成 5 子的奖励较高, 或者连成 5 子的趋势奖励较高. 例如根据棋局情况可以判断当前连成的 3 子在某处落子连成 4 子后必然存在两个位置可以连成 5 子, 即考虑一些必胜情况.

### 策略

通常来说我们初始化全相等 (0时刻每个落子位置都是 $1 slash 100$ 的概率). 此时产生的 $pi$ 形如 $a_t -> "Pr"$. 之后则按照不同的方式进行学习.
